Here's a documentation section for ‚ÄúProcessing‚Äù, based on your provided code. This section explains how the data processing flow works, highlighting important methods and lines of code that demonstrate the logic:


---

üîÑ Processing

The Processing phase in this application is responsible for handling input .txt files from a predefined directory (InputFolder), extracting metadata, parsing the data records based on annotated model classes, and inserting the structured documents into MongoDB collections.

This process is orchestrated primarily within the Main.java class and is broken down into several logical steps:


---

üìÅ 1. Grouping Files by Date

String header = Files.lines(file.toPath()).findFirst().orElse(null);

Each .txt file begins with a header line in the format HDR|dd-MM-yyyy|<ClassName>. The date field is extracted and used to group files.

dateToFileMap.computeIfAbsent(dateStr, k -> new ArrayList<>()).add(file);

Files are stored in a map with the header date as the key, enabling date-wise processing and storage.


---

üìÇ 2. Creating Target Folders by Date

LocalDate date = LocalDate.parse(dateStr, DateTimeFormatter.ofPattern("dd-MM-yyyy"));
Path targetDir = Paths.get("Control", date.format(DateTimeFormatter.ofPattern("yyyy-MM-dd")));
Files.createDirectories(targetDir);

Each group of files is copied into a new subfolder under the Control directory, named by their respective dates. This helps in organizing processed data batches clearly.

Files.copy(file.toPath(), targetFile, StandardCopyOption.REPLACE_EXISTING);


---

üßæ 3. Parsing Each File Using the Model Class

String fileName = lines.get(0).split("\\|")[2];
String className = MODEL_PACKAGE + "." + fileName;
Class<?> modelClass = Class.forName(className);

The third value in the header line is interpreted as the model class name, which will be used for parsing the data lines.


---

üîç 4. Parsing Each Line into a POJO Using Annotations

Object parsed = FixedWidthParser.parseLine(line, modelClass);

The FixedWidthParser class dynamically extracts field values using @FixedFieldAnnotations to enforce constraints like:

Field Length

Required Fields

Allowed Values

Format Validation


Relevant method:

public static <T> T parseLine(String line, Class<T> clazz)


---

‚úÖ 5. Validating and Converting Data Types

Object convertedValue = convertValue(value, field.getType());

Data is converted into Java types like String, Integer, LocalDate, BigDecimal, etc., based on the annotated model field‚Äôs type.


---

üóÉÔ∏è 6. Inserting Records into MongoDB

MongoDBHandler.insertRecords(fileName, details);

Parsed and validated objects are converted to BSON documents using reflection and inserted into the MongoDB collection named after the model class.

collection.insertMany(documents);


---

üìä 7. Maintaining and Verifying Record Counts

public static HashMap<String, Integer> methodcall()

A recordsCount map is maintained to track how many records were inserted per collection. This is verified through unit testing:

assertEquals(value, DbCount);

In TestClass.java, JUnit is used to ensure data consistency by comparing the inserted record count with the actual MongoDB count.


---

‚úÖ Summary

The processing workflow is designed to:

Dynamically route files to their relevant POJO model using metadata.

Enforce data correctness using annotations.

Structure and persist data into MongoDB collections efficiently.

Maintain audit through date-based folder hierarchy and insert count validation.


This flexible and scalable architecture supports adding new data types by simply introducing corresponding model classes, without changing the processing logic.

Let me know if you'd like documentation for any other module like Parsing, Validation, or MongoDB Connectivity.
